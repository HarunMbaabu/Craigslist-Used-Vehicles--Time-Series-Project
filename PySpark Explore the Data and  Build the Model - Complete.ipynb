{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7689a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Craigslist_Vehicles_Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d7f550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming the data is present in the 'data/craigslist_vehicles.csv' file\n",
    "data_path = \"data/craigslist_vehicles.csv\"\n",
    "data = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d559f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+-------+--------------------+-----+------+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+--------------------+--------------------+------+-----+---------+----------+--------------------+--------------------+\n",
      "|   _c0|        id|                 url| region|          region_url|price|  year|manufacturer|model|condition|cylinders|fuel|odometer|title_status|transmission| VIN|drive|size|type|paint_color|           image_url|         description|county|state|      lat|      long|        posting_date|        removal_date|\n",
      "+------+----------+--------------------+-------+--------------------+-----+------+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+--------------------+--------------------+------+-----+---------+----------+--------------------+--------------------+\n",
      "|362773|7307679724|https://abilene.c...|abilene|https://abilene.c...| 4500|2002.0|         bmw|   x5|     null|     null| gas|184000.0|       clean|   automatic|null| null|null|null|       null|https://images.cr...|$4,500 Cash  2002...|  null|   tx|32.401556|-99.884713|2021-04-16 00:00:...|2021-05-02 00:00:...|\n",
      "|362712|7311833696|https://abilene.c...|abilene|https://abilene.c...| 4500|2002.0|         bmw|   x5|     null|     null| gas|184000.0|       clean|   automatic|null| null|null|null|       null|https://images.cr...|$4,500 Cash  2002...|  null|   tx|32.401556|-99.884713|2021-04-24 00:00:...|2021-04-28 00:00:...|\n",
      "+------+----------+--------------------+-------+--------------------+-----+------+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+--------------------+--------------------+------+-----+---------+----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/06 14:17:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , id, url, region, region_url, price, year, manufacturer, model, condition, cylinders, fuel, odometer, title_status, transmission, VIN, drive, size, type, paint_color, image_url, description, county, state, lat, long, posting_date, removal_date\n",
      " Schema: _c0, id, url, region, region_url, price, year, manufacturer, model, condition, cylinders, fuel, odometer, title_status, transmission, VIN, drive, size, type, paint_color, image_url, description, county, state, lat, long, posting_date, removal_date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/grayhat/Desktop/int-athena/data/craigslist_vehicles.csv\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "157f3a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'id',\n",
       " 'url',\n",
       " 'region',\n",
       " 'region_url',\n",
       " 'price',\n",
       " 'year',\n",
       " 'manufacturer',\n",
       " 'model',\n",
       " 'condition',\n",
       " 'cylinders',\n",
       " 'fuel',\n",
       " 'odometer',\n",
       " 'title_status',\n",
       " 'transmission',\n",
       " 'VIN',\n",
       " 'drive',\n",
       " 'size',\n",
       " 'type',\n",
       " 'paint_color',\n",
       " 'image_url',\n",
       " 'description',\n",
       " 'county',\n",
       " 'state',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'posting_date',\n",
       " 'removal_date']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5218b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'url', 'region_url', 'VIN', 'image_url', 'description', 'county', 'lat', 'long', 'removal_date']\n",
    "\n",
    "# Filter the columns to drop only those that exist in the DataFrame\n",
    "columns_to_drop_existing = [col for col in columns_to_drop if col in data.columns]\n",
    "\n",
    "# Drop the existing columns\n",
    "data = data.drop(*columns_to_drop_existing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc83c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('posting_date', col('posting_date').cast(TimestampType()).alias('posting_date'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52978c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    # Fill missing numerical values with mean\n",
    "    numerical_columns = ['year', 'odometer']\n",
    "    for column in numerical_columns:\n",
    "        mean_value = data.selectExpr(f'avg({column})').collect()[0][0]\n",
    "        data = data.withColumn(column, F.when(F.col(column).isNull(), mean_value).otherwise(F.col(column)))\n",
    "\n",
    "    # Fill missing categorical values with mode\n",
    "    categorical_columns = ['manufacturer', 'model', 'condition', 'cylinders', 'fuel', 'title_status',\n",
    "                           'transmission', 'drive', 'size', 'type', 'paint_color', 'posting_date']\n",
    "    for column in categorical_columns:\n",
    "        mode_value = data.groupBy(column).count().orderBy(F.col('count').desc()).first()[column]\n",
    "        data = data.withColumn(column, F.when(F.col(column).isNull(), mode_value).otherwise(F.col(column)))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = handle_missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f2558",
   "metadata": {},
   "source": [
    "### **Spark will save the cleaned data in partations which we can easily use creating Time series Models.** \n",
    "\n",
    "- Since i  the modeling in the \"Pandas Explore the Data and Build the Model - Complete.ipynb\" i will stop this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077972bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
